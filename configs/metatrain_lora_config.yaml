defaults:
  - dataset: archivalqa

debug: false

seed: 7
base_model: distilgpt2
base_model_state_dict: /iris/u/nathu/un_camels/pretrained_models/archivalqa_paragraphs/distilgpt2.pt

weight_model_base: distilgpt2
weight_model_freeze_base: false
weight_model_nl: sigmoid

r: 4 #rank for LORA
lora_model_path: ???

batch_size: 6
num_epochs: 50

reset_base_frequency: 8
gradient_accumulation_steps: 8
validation_frequency: 240 #how often we validate, in terms of model updates (batchs*gradient_accumulation_steps)

outer_lr: 2.5e-5
inner_lr: 1e-4
learn_inner_lr: true

inner_lora: false
gradient_clip_threshold: 10

notes: ''

hydra:
  job:
    chdir: true
  run:  
    dir: outputs/metatrain_lora/${dataset.name}/${now:%Y-%m-%d}_${now:%H:%M:%S}:${notes}
  